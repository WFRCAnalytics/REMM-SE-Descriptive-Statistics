{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import arcpy\n",
    "# from arcpy import env\n",
    "import os\n",
    "import numpy as np\n",
    "import keyring\n",
    "from arcgis import GIS\n",
    "from arcgis.features import GeoAccessor\n",
    "from arcgis.features import GeoSeriesAccessor\n",
    "# import fiona # used to import gdbs \n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# arcpy.env.overwriteOutput = True\n",
    "# arcpy.env.parallelProcessingFactor = \"90%\"\n",
    "\n",
    "# show all columns\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# pd.DataFrame.spatial.from_featureclass(???)  \n",
    "# df.spatial.to_featureclass(location=???,sanitize_columns=False)  \n",
    "\n",
    "# gsa = arcgis.features.GeoSeriesAccessor(df['SHAPE'])  \n",
    "# df['AREA'] = gsa.area  # KNOW YOUR UNITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sign into ArcGIS Online\n",
    "un = 'analytics_wfrc'\n",
    "pw = keyring.get_password('Analytics AGOL', un)\n",
    "gis = GIS(username=un, password=pw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_pickle(r\"\\\\server1\\Volumef\\SHARED\\Josh\\2023-Official-Forecast-Files\\mq196\\Progression_Metrics\\run_196_year_2019_parcel_progression_metrics.pkl\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NA values in Spatially enabled dataframes (ignores SHAPE column)\n",
    "def fill_na_sedf(df_with_shape_column, fill_value=0):\n",
    "    if 'SHAPE' in list(df_with_shape_column.columns):\n",
    "        df = df_with_shape_column.copy()\n",
    "        shape_column = df['SHAPE'].copy()\n",
    "        del df['SHAPE']\n",
    "        return df.fillna(fill_value).merge(shape_column,left_index=True, right_index=True, how='inner')\n",
    "    else:\n",
    "        raise Exception(\"Dataframe does not include 'SHAPE' column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_year = 2019\n",
    "# base_year = 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if base_year == 2015:\n",
    "    outputs_pkl = r'.\\\\Outputs\\progession_metrics_2015_pkl'\n",
    "    outputs_csv = r'.\\\\Outputs\\progession_metrics_2015_csv'\n",
    "if base_year == 2019:\n",
    "    outputs_pkl = r'.\\\\Outputs\\progession_metrics_pkl'\n",
    "    outputs_csv = r'.\\\\Outputs\\progession_metrics_csv'\n",
    "\n",
    "for d in [outputs_pkl, outputs_csv]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parcel Equivalency Table\n",
    "if base_year == 2015:\n",
    "    eq = pd.read_csv(r\".\\Inputs\\parcel_eq_2015_v1.csv\")\n",
    "if base_year == 2019:\n",
    "    eq = pd.read_csv(r\".\\Inputs\\parcel_eq_v7.csv\")\n",
    "    \n",
    "# centers_eq_ids = eq[eq['CENTER_NAME'].isna() == False]['parcel_id'].to_list()\n",
    "\n",
    "# # centers shape\n",
    "# centers_sdf = pd.DataFrame.spatial.from_featureclass(r\".\\Inputs\\WC_2050_Centers.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if base_year == 2015:\n",
    "    remm_folder_1 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\REMM Runs\\Progression_Metrics_2015_1\"\n",
    "    remm_folder_2 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\REMM Runs\\Progression_Metrics_2015_2\"\n",
    "    remm_folder_3 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\REMM Runs\\Progression_Metrics_2015_3\"\n",
    "    remm_folder_4 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\REMM Runs\\Progression_Metrics_2015_4\"\n",
    "    remm_folder_5 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\REMM Runs\\Progression_Metrics_2015_5\"\n",
    "    remm_folder_6 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\REMM Runs\\Progression_Metrics_2015_6\"\n",
    "\n",
    "if base_year == 2019:\n",
    "    remm_folder_1 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\2023-Official-Forecast-Files\\andy231\\Progression_Metrics\"\n",
    "    remm_folder_2 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\2023-Official-Forecast-Files\\mq196\\Progression_Metrics\"\n",
    "    remm_folder_3 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\2023-Official-Forecast-Files\\mq238\\Progression_Metrics\"\n",
    "    remm_folder_4 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\2023-Official-Forecast-Files\\mq273\\Progression_Metrics\"\n",
    "    remm_folder_5 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\2023-Official-Forecast-Files\\mq339\\Progression_Metrics\"\n",
    "    remm_folder_6 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\2023-Official-Forecast-Files\\mq376\\Progression_Metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "remm_progression_folders = [remm_folder_1, remm_folder_2, remm_folder_3, remm_folder_4, remm_folder_5, remm_folder_6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # csv version\n",
    "# def get_table_ignore_base(path, year):\n",
    "#     csvs = glob.glob(os.path.join(path, f'run_*_year_{year}_parcel_progression_metrics.csv'))\n",
    "#     csvs = [csv for csv in csvs if 'base'not in csv]\n",
    "#     if len(csvs) > 1:\n",
    "#         print('warning multiple tables were globbed; only the first will be returned')\n",
    "#     return pd.read_csv(csvs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl version\n",
    "def get_table_ignore_base(path, year):\n",
    "    pkls = glob.glob(os.path.join(path, f'run_*_year_{year}_parcel_progression_metrics.pkl'))\n",
    "    pkls = [pkl for pkl in pkls if 'base'not in pkl]\n",
    "    if len(pkls) > 1:\n",
    "        pkls.sort(reverse=True)\n",
    "        print('Warning: multiple tables were globbed; only the first will be returned')\n",
    "    return pd.read_pickle(pkls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df, year):\n",
    "    df = df.set_index('parcel_id')\n",
    "    df.loc[(df['is_sf']==1), 'sf_units'] = df['residential_units']\n",
    "    df.loc[(df['is_mf']==1), 'mf_units'] = df['residential_units']\n",
    "    df['industrial_jobs'] = df['jobs_wholesale'] + df['jobs_manuf']\n",
    "    df['retail_jobs'] = df['jobs_retail'] + df['jobs_accom_food']\n",
    "    df['office_jobs'] = df['jobs_office'] + df['jobs_gov_edu'] + df['jobs_health'] + df['jobs_other']\n",
    "    df.loc[(df['has_buildings'] != 1), 'vacant_acres'] = df['parcel_acres']\n",
    "    df.loc[(df['has_buildings'] != 1) & (df['developable'] == 1), 'vacant_devacres'] = df['parcel_acres']\n",
    "    df['vacant_acres'].fillna(0, inplace=True)\n",
    "    df['vacant_devacres'].fillna(0, inplace=True)\n",
    "    df['households'] = df['households_count']\n",
    "\n",
    "    # df = df[['sf_units', 'mf_units', 'households', 'hhpop', \n",
    "    #          'job_spaces', 'industrial_jobs', 'retail_jobs', 'office_jobs', \n",
    "    #          'vacant_acres', 'vacant_devacres', 'non_res_value', 'res_value', 'total_value', 'non_residential_sqft']].copy()\n",
    "    \n",
    "    df = df[['sf_units', 'mf_units', 'households', 'hhpop', \n",
    "             'industrial_jobs', 'retail_jobs', 'office_jobs',\n",
    "             'job_spaces', 'jobs_accom_food',\n",
    "             'jobs_gov_edu', 'jobs_health', 'jobs_manuf', 'jobs_office',\n",
    "             'jobs_other', 'jobs_retail', 'jobs_wholesale', \n",
    "             'vacant_acres', 'vacant_devacres', 'non_res_value', 'res_value', 'total_value', 'non_residential_sqft']].copy()\n",
    "\n",
    "    return df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if base_year == 2019:\n",
    "    statewide_se  = gis.content.get('8577570d0df6451f90bbd52b2bd3461c').tables[0].query(where='1=1').sdf # ~4-min load\n",
    "    wf_se = statewide_se[statewide_se['SUBAREAID'] == 1].copy()\n",
    "    wf_se = wf_se[['YEAR','SA_TAZID', 'TOTHH', 'HHPOP', 'RETEMP','INDEMP','OTHEMP','TYPEMP', 'RETL', 'FOOD', 'MANU', 'WSLE', 'OFFI', 'GVED', 'HLTH', 'OTHR']].copy() # retail, industrial, office, typical\n",
    "    wf_se.rename({'SA_TAZID':'TAZID_900'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019\n",
      "2022\n",
      "2025\n",
      "2028\n",
      "2031\n",
      "2034\n",
      "2037\n",
      "2040\n",
      "2043\n",
      "2046\n",
      "2049\n"
     ]
    }
   ],
   "source": [
    "# base = centers_sdf[['CenterName', 'DEVACRES', 'SHAPE']].copy()\n",
    "for year in range(base_year,2051):\n",
    "    \n",
    "    if year % 3 == 0:\n",
    "        print(year)\n",
    "\n",
    "    dfs_current_year = [get_table_ignore_base(f, year) for f in remm_progression_folders]\n",
    "    dfs_processed = [prepare_df(df, year) for df in dfs_current_year] \n",
    "\n",
    "    # stack average the 6 runs together\n",
    "    data_stack = pd.concat(dfs_processed)\n",
    "\n",
    "    # calculate median\n",
    "    median_vars =  ['job_spaces' , 'vacant_acres', 'vacant_devacres', 'non_res_value', 'res_value', 'total_value', 'non_residential_sqft']\n",
    "    median = data_stack.groupby(data_stack.index).median().reset_index()\n",
    "    median = median[median_vars]\n",
    "\n",
    "    # calculate average\n",
    "    average = data_stack.groupby(data_stack.index).mean().reset_index()\n",
    "    average.drop(median_vars, axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    # determine taz id field\n",
    "    if base_year == 2015:\n",
    "        taz_id_field = 'TAZID_832'\n",
    "    if base_year == 2019:\n",
    "        taz_id_field = 'TAZID_900'\n",
    "    \n",
    "    # join taz id to the average table\n",
    "    average = average.merge(eq[['parcel_id', 'TAZID_900']], on='parcel_id', how='left')\n",
    "    wf_se_year = wf_se[wf_se['YEAR'] == year].copy()\n",
    "    \n",
    "    # join se data to the average table\n",
    "    average = average.merge(wf_se_year, on='TAZID_900', how='left')\n",
    "    \n",
    "    # sum the parcel se variables by tazid\n",
    "    average_taz_sum = average.groupby('TAZID_900')[['households', 'hhpop', \n",
    "             'industrial_jobs', 'retail_jobs', 'office_jobs', 'jobs_accom_food',\n",
    "             'jobs_gov_edu', 'jobs_health', 'jobs_manuf', 'jobs_office',\n",
    "             'jobs_other', 'jobs_retail', 'jobs_wholesale']].sum()\n",
    "    \n",
    "    # rename the columns\n",
    "    average_taz_sum.columns = [x + '_sum' for x in average_taz_sum.columns]\n",
    "    average_taz_sum = average_taz_sum.reset_index()\n",
    "\n",
    "    # add taz sum and median variables to the average table\n",
    "    average = average.merge(average_taz_sum, on='TAZID_900', how='left')\n",
    "    average = average.merge(median, left_index=True, right_index=True, how='left')\n",
    "\n",
    "    # adjust se variables using official taz forecast\n",
    "    average['households'] = (average['households'] / average['households_sum']) * average['TOTHH'] \n",
    "    average['hhpop'] = (average['hhpop'] / average['hhpop_sum']) * average['HHPOP'] \n",
    "    average['industrial_jobs'] = (average['industrial_jobs'] / average['industrial_jobs_sum']) * average['INDEMP'] \n",
    "    average['retail_jobs'] = (average['retail_jobs'] / average['retail_jobs_sum']) * average['RETEMP'] \n",
    "    average['office_jobs'] = (average['office_jobs'] / average['office_jobs_sum']) * average['OTHEMP'] \n",
    "    average['jobs_accom_food'] = (average['jobs_accom_food'] / average['jobs_accom_food_sum']) * average['FOOD'] \n",
    "    average['jobs_gov_edu'] = (average['jobs_gov_edu'] / average['jobs_gov_edu_sum']) * average['GVED'] \n",
    "    average['jobs_health'] = (average['jobs_health'] / average['jobs_health_sum']) * average['HLTH'] \n",
    "    average['jobs_manuf'] = (average['jobs_manuf'] / average['jobs_manuf_sum']) * average['MANU'] \n",
    "    average['jobs_office'] = (average['jobs_office'] / average['jobs_office_sum']) * average['OFFI'] \n",
    "    average['jobs_other'] = (average['jobs_other'] / average['jobs_other_sum']) * average['OTHR'] \n",
    "    average['jobs_retail'] = (average['jobs_retail'] / average['jobs_retail_sum']) * average['RETL'] \n",
    "    average['jobs_wholesale'] = (average['jobs_wholesale'] / average['jobs_wholesale_sum']) * average['WSLE'] \n",
    "\n",
    "    # fill na\n",
    "    average = average.fillna(0)\n",
    "\n",
    "    # recalculate total jobs and res units\n",
    "    average['total_jobs'] = average['office_jobs'] + average['retail_jobs'] + average['industrial_jobs']\n",
    "    average['residential_units'] = average['sf_units'] + average['mf_units']\n",
    "    \n",
    "    average = average[['parcel_id','sf_units','mf_units','households','hhpop',\n",
    "                       'industrial_jobs','retail_jobs','office_jobs','job_spaces','jobs_accom_food',\n",
    "                       'jobs_gov_edu','jobs_health','jobs_manuf','jobs_office','jobs_other',\n",
    "                       'jobs_retail','jobs_wholesale','vacant_acres','vacant_devacres','non_res_value',\n",
    "                       'res_value','total_value','non_residential_sqft','residential_units','total_jobs']].copy()\n",
    "\n",
    "    # export\n",
    "    if base_year == 2015:\n",
    "        average.to_pickle(os.path.join(outputs_pkl, f'parcel_se_b2015_{year}.pkl'))\n",
    "        average.to_csv(os.path.join(outputs_csv, f'parcel_se_b2015_{year}.csv'), index=False)\n",
    "    if base_year == 2019:\n",
    "        average.to_pickle(os.path.join(outputs_pkl, f'parcel_se_{year}.pkl'))\n",
    "        average.to_csv(os.path.join(outputs_csv, f'parcel_se_{year}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcel_id</th>\n",
       "      <th>sf_units</th>\n",
       "      <th>mf_units</th>\n",
       "      <th>households</th>\n",
       "      <th>hhpop</th>\n",
       "      <th>industrial_jobs</th>\n",
       "      <th>retail_jobs</th>\n",
       "      <th>office_jobs</th>\n",
       "      <th>job_spaces</th>\n",
       "      <th>jobs_accom_food</th>\n",
       "      <th>jobs_gov_edu</th>\n",
       "      <th>jobs_health</th>\n",
       "      <th>jobs_manuf</th>\n",
       "      <th>jobs_office</th>\n",
       "      <th>jobs_other</th>\n",
       "      <th>jobs_retail</th>\n",
       "      <th>jobs_wholesale</th>\n",
       "      <th>vacant_acres</th>\n",
       "      <th>vacant_devacres</th>\n",
       "      <th>non_res_value</th>\n",
       "      <th>res_value</th>\n",
       "      <th>total_value</th>\n",
       "      <th>non_residential_sqft</th>\n",
       "      <th>residential_units</th>\n",
       "      <th>total_jobs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38738</th>\n",
       "      <td>40896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>995.835662</td>\n",
       "      <td>1454.160479</td>\n",
       "      <td>2450.0</td>\n",
       "      <td>278.333333</td>\n",
       "      <td>22.498093</td>\n",
       "      <td>1.999412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>829.498361</td>\n",
       "      <td>600.166667</td>\n",
       "      <td>717.503333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.199336e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.199346e+08</td>\n",
       "      <td>2912147.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2449.996141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       parcel_id  sf_units  mf_units  households  hhpop  industrial_jobs  \\\n",
       "38738      40896       0.0       0.0         0.0    0.0              0.0   \n",
       "\n",
       "       retail_jobs  office_jobs  job_spaces  jobs_accom_food  jobs_gov_edu  \\\n",
       "38738   995.835662  1454.160479      2450.0       278.333333     22.498093   \n",
       "\n",
       "       jobs_health  jobs_manuf  jobs_office  jobs_other  jobs_retail  \\\n",
       "38738     1.999412         0.0   829.498361  600.166667   717.503333   \n",
       "\n",
       "       jobs_wholesale  vacant_acres  vacant_devacres  non_res_value  \\\n",
       "38738             0.0           0.0              0.0   4.199336e+08   \n",
       "\n",
       "       res_value   total_value  non_residential_sqft  residential_units  \\\n",
       "38738        0.0  4.199346e+08             2912147.0                0.0   \n",
       "\n",
       "        total_jobs  \n",
       "38738  2449.996141  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average['total_jobs'] = average['office_jobs'] + average['retail_jobs'] + average['industrial_jobs']\n",
    "average[average['parcel_id']==40896]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# unpickled_df = pd.read_pickle(r\"\\\\server1\\Volumef\\SHARED\\Josh\\2023-Official-Forecast-Files\\mq376\\Progression_Metrics\\run_376_year_2019_parcel_progression_metrics.pkl\")\n",
    "# unpickled_df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3245673af07dcc28bdd829afb187282e9288a1f8195a5928b70ecba6e5973721"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
