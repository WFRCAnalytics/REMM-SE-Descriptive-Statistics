{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import arcpy\n",
    "# from arcpy import env\n",
    "import os\n",
    "import numpy as np\n",
    "import keyring\n",
    "from arcgis import GIS\n",
    "from arcgis.features import GeoAccessor\n",
    "from arcgis.features import GeoSeriesAccessor\n",
    "# import fiona # used to import gdbs \n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# arcpy.env.overwriteOutput = True\n",
    "# arcpy.env.parallelProcessingFactor = \"90%\"\n",
    "\n",
    "# show all columns\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# pd.DataFrame.spatial.from_featureclass(???)  \n",
    "# df.spatial.to_featureclass(location=???,sanitize_columns=False)  \n",
    "\n",
    "# gsa = arcgis.features.GeoSeriesAccessor(df['SHAPE'])  \n",
    "# df['AREA'] = gsa.area  # KNOW YOUR UNITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sign into ArcGIS Online\n",
    "un = 'analytics_wfrc'\n",
    "pw = keyring.get_password('Analytics AGOL', un)\n",
    "gis = GIS(username=un, password=pw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['parcel_id', 'county_id', 'zone_id', 'parcel_acres', 'land_value',\n",
       "       'max_far', 'max_dua', 'has_buildings', 'was_developed',\n",
       "       'was_redeveloped', 'developable', 'residential_units', 'job_spaces',\n",
       "       'building_sqft', 'non_residential_sqft', 'residential_sqft',\n",
       "       'unit_price_non_residential', 'res_price_per_sqft', 'is_sf', 'is_mf',\n",
       "       'is_industrial', 'is_retail', 'is_office', 'is_govt', 'is_mixeduse',\n",
       "       'is_other', 'year_built', 'building_count', 'jobs_accom_food',\n",
       "       'jobs_gov_edu', 'jobs_health', 'jobs_manuf', 'jobs_office',\n",
       "       'jobs_other', 'jobs_retail', 'jobs_wholesale', 'households_count',\n",
       "       'hhpop', 'non_res_value', 'res_value', 'total_value',\n",
       "       'job_spaces_added', 'jobs_accom_food_added', 'jobs_gov_edu_added',\n",
       "       'jobs_health_added', 'jobs_manuf_added', 'jobs_office_added',\n",
       "       'jobs_other_added', 'jobs_retail_added', 'jobs_wholesale_added',\n",
       "       'res_units_added', 'households_added', 'acreage_dev', 'acreage_redev',\n",
       "       'acreage_dev_res', 'acreage_redev_res', 'acreage_dev_nonres',\n",
       "       'acreage_redev_nonres', 'res_units_added_dev', 'res_units_added_redev',\n",
       "       'value_added_dev', 'value_added_redev', 'value_added_dev_res',\n",
       "       'value_added_dev_nonres'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle(r\"\\\\server1\\Volumef\\SHARED\\Josh\\2023-Official-Forecast-Files\\mq196\\Progression_Metrics\\run_196_year_2019_parcel_progression_metrics.pkl\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NA values in Spatially enabled dataframes (ignores SHAPE column)\n",
    "def fill_na_sedf(df_with_shape_column, fill_value=0):\n",
    "    if 'SHAPE' in list(df_with_shape_column.columns):\n",
    "        df = df_with_shape_column.copy()\n",
    "        shape_column = df['SHAPE'].copy()\n",
    "        del df['SHAPE']\n",
    "        return df.fillna(fill_value).merge(shape_column,left_index=True, right_index=True, how='inner')\n",
    "    else:\n",
    "        raise Exception(\"Dataframe does not include 'SHAPE' column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_year = 2019\n",
    "# base_year = 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if base_year == 2015:\n",
    "    outputs_pkl = r'.\\\\Outputs\\progession_metrics_2015_pkl'\n",
    "    outputs_csv = r'.\\\\Outputs\\progession_metrics_2015_csv'\n",
    "if base_year == 2019:\n",
    "    outputs_pkl = r'.\\\\Outputs\\progession_metrics_pkl'\n",
    "    outputs_csv = r'.\\\\Outputs\\progession_metrics_csv'\n",
    "\n",
    "for d in [outputs_pkl, outputs_csv]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parcel Equivalency Table\n",
    "if base_year == 2015:\n",
    "    eq = pd.read_csv(r\".\\Inputs\\parcel_eq_2015_v1.csv\")\n",
    "if base_year == 2019:\n",
    "    eq = pd.read_csv(r\".\\Inputs\\parcel_eq_v7.csv\")\n",
    "    \n",
    "# centers_eq_ids = eq[eq['CENTER_NAME'].isna() == False]['parcel_id'].to_list()\n",
    "\n",
    "# # centers shape\n",
    "# centers_sdf = pd.DataFrame.spatial.from_featureclass(r\".\\Inputs\\WC_2050_Centers.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if base_year == 2015:\n",
    "    remm_folder_1 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\REMM Runs\\Progression_Metrics_2015_1\"\n",
    "    remm_folder_2 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\REMM Runs\\Progression_Metrics_2015_2\"\n",
    "    remm_folder_3 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\REMM Runs\\Progression_Metrics_2015_3\"\n",
    "    remm_folder_4 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\REMM Runs\\Progression_Metrics_2015_4\"\n",
    "    remm_folder_5 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\REMM Runs\\Progression_Metrics_2015_5\"\n",
    "    remm_folder_6 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\REMM Runs\\Progression_Metrics_2015_6\"\n",
    "\n",
    "if base_year == 2019:\n",
    "    remm_folder_1 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\2023-Official-Forecast-Files\\andy231\\Progression_Metrics\"\n",
    "    remm_folder_2 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\2023-Official-Forecast-Files\\mq196\\Progression_Metrics\"\n",
    "    remm_folder_3 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\2023-Official-Forecast-Files\\mq238\\Progression_Metrics\"\n",
    "    remm_folder_4 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\2023-Official-Forecast-Files\\mq273\\Progression_Metrics\"\n",
    "    remm_folder_5 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\2023-Official-Forecast-Files\\mq339\\Progression_Metrics\"\n",
    "    remm_folder_6 = r\"\\\\server1\\Volumef\\SHARED\\Josh\\2023-Official-Forecast-Files\\mq376\\Progression_Metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "remm_progression_folders = [remm_folder_1, remm_folder_2, remm_folder_3, remm_folder_4, remm_folder_5, remm_folder_6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # csv version\n",
    "# def get_table_ignore_base(path, year):\n",
    "#     csvs = glob.glob(os.path.join(path, f'run_*_year_{year}_parcel_progression_metrics.csv'))\n",
    "#     csvs = [csv for csv in csvs if 'base'not in csv]\n",
    "#     if len(csvs) > 1:\n",
    "#         print('warning multiple tables were globbed; only the first will be returned')\n",
    "#     return pd.read_csv(csvs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl version\n",
    "def get_table_ignore_base(path, year):\n",
    "    pkls = glob.glob(os.path.join(path, f'run_*_year_{year}_parcel_progression_metrics.pkl'))\n",
    "    pkls = [pkl for pkl in pkls if 'base'not in pkl]\n",
    "    if len(pkls) > 1:\n",
    "        pkls.sort(reverse=True)\n",
    "        print('Warning: multiple tables were globbed; only the first will be returned')\n",
    "    return pd.read_pickle(pkls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df, year):\n",
    "    df = df.set_index('parcel_id')\n",
    "    df.loc[(df['is_sf']==1), 'sf_units'] = df['residential_units']\n",
    "    df.loc[(df['is_mf']==1), 'mf_units'] = df['residential_units']\n",
    "    df['industrial_jobs'] = df['jobs_wholesale'] + df['jobs_manuf']\n",
    "    df['retail_jobs'] = df['jobs_retail'] + df['jobs_accom_food']\n",
    "    df['office_jobs'] = df['jobs_office'] + df['jobs_gov_edu'] + df['jobs_health'] + df['jobs_other']\n",
    "    df.loc[(df['has_buildings'] != 1), 'vacant_acres'] = df['parcel_acres']\n",
    "    df.loc[(df['has_buildings'] != 1) & (df['developable'] == 1), 'vacant_devacres'] = df['parcel_acres']\n",
    "    df['vacant_acres'].fillna(0, inplace=True)\n",
    "    df['vacant_devacres'].fillna(0, inplace=True)\n",
    "    df['households'] = df['households_count']\n",
    "\n",
    "    # df = df[['sf_units', 'mf_units', 'households', 'hhpop', \n",
    "    #          'job_spaces', 'industrial_jobs', 'retail_jobs', 'office_jobs', \n",
    "    #          'vacant_acres', 'vacant_devacres', 'non_res_value', 'res_value', 'total_value', 'non_residential_sqft']].copy()\n",
    "    \n",
    "    df = df[['sf_units', 'mf_units', 'households', 'hhpop', \n",
    "             'industrial_jobs', 'retail_jobs', 'office_jobs',\n",
    "             'job_spaces', 'jobs_accom_food',\n",
    "             'jobs_gov_edu', 'jobs_health', 'jobs_manuf', 'jobs_office',\n",
    "             'jobs_other', 'jobs_retail', 'jobs_wholesale', \n",
    "             'vacant_acres', 'vacant_devacres', 'non_res_value', 'res_value', 'total_value', 'non_residential_sqft']].copy()\n",
    "\n",
    "    return df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'statewide_se' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12328\\3213790592.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mbase_year\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2019\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# statewide_se  = gis.content.get('8577570d0df6451f90bbd52b2bd3461c').tables[0].query(where='1=1').sdf # ~4-min load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mwf_se\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstatewide_se\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstatewide_se\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SUBAREAID'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mwf_se\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwf_se\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'YEAR'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'SA_TAZID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TOTHH'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'HHPOP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RETEMP'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'INDEMP'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'OTHEMP'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'TYPEMP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RETL'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'FOOD'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'MANU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'WSLE'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'OFFI'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GVED'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'HLTH'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'OTHR'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# retail, industrial, office, typical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwf_se\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'SA_TAZID'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'TAZID_900'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'statewide_se' is not defined"
     ]
    }
   ],
   "source": [
    "if base_year == 2019:\n",
    "    # statewide_se  = gis.content.get('8577570d0df6451f90bbd52b2bd3461c').tables[0].query(where='1=1').sdf # ~4-min load\n",
    "    wf_se = statewide_se[statewide_se['SUBAREAID'] == 1].copy()\n",
    "    wf_se = wf_se[['YEAR','SA_TAZID', 'TOTHH', 'HHPOP', 'RETEMP','INDEMP','OTHEMP','TYPEMP', 'RETL', 'FOOD', 'MANU', 'WSLE', 'OFFI', 'GVED', 'HLTH', 'OTHR']].copy() # retail, industrial, office, typical\n",
    "    wf_se.rename({'SA_TAZID':'TAZID_900'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019\n",
      "2022\n",
      "2025\n",
      "2028\n",
      "2031\n",
      "2034\n",
      "2037\n",
      "2040\n",
      "2043\n",
      "2046\n",
      "2049\n"
     ]
    }
   ],
   "source": [
    "# base = centers_sdf[['CenterName', 'DEVACRES', 'SHAPE']].copy()\n",
    "for year in range(base_year,2051):\n",
    "    \n",
    "    if year % 3 == 0:\n",
    "        print(year)\n",
    "\n",
    "    dfs_current_year = [get_table_ignore_base(f, year) for f in remm_progression_folders]\n",
    "    dfs_processed = [prepare_df(df, year) for df in dfs_current_year] \n",
    "\n",
    "    # stack average the 6 runs together\n",
    "    data_stack = pd.concat(dfs_processed)\n",
    "\n",
    "    # calculate median\n",
    "    median_vars =  ['job_spaces' , 'vacant_acres', 'vacant_devacres', 'non_res_value', 'res_value', 'total_value', 'non_residential_sqft']\n",
    "    median = data_stack.groupby(data_stack.index).median().reset_index()\n",
    "    median = median[median_vars]\n",
    "\n",
    "    # calculate average\n",
    "    average = data_stack.groupby(data_stack.index).mean().reset_index()\n",
    "    average.drop(median_vars, axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    # determine taz id field\n",
    "    if base_year == 2015:\n",
    "        taz_id_field = 'TAZID_832'\n",
    "    if base_year == 2019:\n",
    "        taz_id_field = 'TAZID_900'\n",
    "    \n",
    "    # join taz id to the average table\n",
    "    average = average.merge(eq[['parcel_id', 'TAZID_900']], on='parcel_id', how='left')\n",
    "    wf_se_year = wf_se[wf_se['YEAR'] == year].copy()\n",
    "    \n",
    "    # join se data to the average table\n",
    "    average = average.merge(wf_se_year, on='TAZID_900', how='left')\n",
    "    \n",
    "    # sum the parcel se variables by tazid\n",
    "    average_taz_sum = average.groupby('TAZID_900')[['households', 'hhpop', \n",
    "             'industrial_jobs', 'retail_jobs', 'office_jobs', 'jobs_accom_food',\n",
    "             'jobs_gov_edu', 'jobs_health', 'jobs_manuf', 'jobs_office',\n",
    "             'jobs_other', 'jobs_retail', 'jobs_wholesale']].sum()\n",
    "    \n",
    "    # rename the columns\n",
    "    average_taz_sum.columns = [x + '_sum' for x in average_taz_sum.columns]\n",
    "    average_taz_sum = average_taz_sum.reset_index()\n",
    "\n",
    "    # add taz sum and median variables to the average table\n",
    "    average = average.merge(average_taz_sum, on='TAZID_900', how='left')\n",
    "    average = average.merge(median, left_index=True, right_index=True, how='left')\n",
    "\n",
    "    # adjust se variables using official taz forcast\n",
    "    average['households'] = (average['households'] / average['households_sum']) * average['TOTHH'] \n",
    "    average['hhpop'] = (average['hhpop'] / average['hhpop_sum']) * average['HHPOP'] \n",
    "    average['industrial_jobs'] = (average['industrial_jobs'] / average['industrial_jobs_sum']) * average['INDEMP'] \n",
    "    average['retail_jobs'] = (average['retail_jobs'] / average['retail_jobs_sum']) * average['RETEMP'] \n",
    "    average['office_jobs'] = (average['office_jobs'] / average['office_jobs_sum']) * average['OTHEMP'] \n",
    "    average['jobs_accom_food'] = (average['jobs_accom_food'] / average['jobs_accom_food_sum']) * average['FOOD'] \n",
    "    average['jobs_gov_edu'] = (average['jobs_gov_edu'] / average['jobs_gov_edu_sum']) * average['GVED'] \n",
    "    average['jobs_health'] = (average['jobs_health'] / average['jobs_health_sum']) * average['HLTH'] \n",
    "    average['jobs_manuf'] = (average['jobs_manuf'] / average['jobs_manuf_sum']) * average['MANU'] \n",
    "    average['jobs_office'] = (average['jobs_office'] / average['jobs_office_sum']) * average['OFFI'] \n",
    "    average['jobs_other'] = (average['jobs_other'] / average['jobs_other_sum']) * average['OTHR'] \n",
    "    average['jobs_retail'] = (average['jobs_retail'] / average['jobs_retail_sum']) * average['RETL'] \n",
    "    average['jobs_wholesale'] = (average['jobs_wholesale'] / average['jobs_wholesale_sum']) * average['WSLE'] \n",
    "\n",
    "    # recalcualte total jobs and res units\n",
    "    average['total_jobs'] = average['office_jobs'] + average['retail_jobs'] + average['industrial_jobs']\n",
    "    average['residential_units'] = average['sf_units'] + average['mf_units']\n",
    "\n",
    "    # fill NAs\n",
    "    average = average.fillna(0)\n",
    "    average = average[['parcel_id','sf_units','mf_units','households','hhpop',\n",
    "                       'industrial_jobs','retail_jobs','office_jobs','job_spaces','jobs_accom_food',\n",
    "                       'jobs_gov_edu','jobs_health','jobs_manuf','jobs_office','jobs_other',\n",
    "                       'jobs_retail','jobs_wholesale','vacant_acres','vacant_devacres','non_res_value',\n",
    "                       'res_value','total_value','non_residential_sqft','residential_units','total_jobs']]\n",
    "\n",
    "    # export\n",
    "    if base_year == 2015:\n",
    "        average.to_pickle(os.path.join(outputs_pkl, f'parcel_se_b2015_{year}.pkl'))\n",
    "        average.to_csv(os.path.join(outputs_csv, f'parcel_se_b2015_{year}.csv'), index=False)\n",
    "    if base_year == 2019:\n",
    "        average.to_pickle(os.path.join(outputs_pkl, f'parcel_se_{year}.pkl'))\n",
    "        average.to_csv(os.path.join(outputs_csv, f'parcel_se_{year}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# unpickled_df = pd.read_pickle(r\"\\\\server1\\Volumef\\SHARED\\Josh\\2023-Official-Forecast-Files\\mq376\\Progression_Metrics\\run_376_year_2019_parcel_progression_metrics.pkl\")\n",
    "# unpickled_df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3245673af07dcc28bdd829afb187282e9288a1f8195a5928b70ecba6e5973721"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
